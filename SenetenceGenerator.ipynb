{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a36769-9811-4e56-a55c-e195b08b560f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK data...\n",
      "TextBlob corpora download failed - using fallback correction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Denin\n",
      "[nltk_data]     Thomas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Denin\n",
      "[nltk_data]     Thomas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import nltk\n",
    "import subprocess\n",
    "from textblob import TextBlob\n",
    "\n",
    "# ================== NLTK DATA SETUP ================== #\n",
    "def download_nltk_data():\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "        nltk.data.find('corpora/wordnet')\n",
    "    except LookupError:\n",
    "        print(\"Downloading NLTK data...\")\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('wordnet')\n",
    "        try:\n",
    "            subprocess.run([\"python\", \"-m\", \"textblob.download_corpora\"], check=True)\n",
    "        except:\n",
    "            print(\"TextBlob corpora download failed - using fallback correction\")\n",
    "\n",
    "download_nltk_data()\n",
    "\n",
    "# ================== MODEL SETUP ================== #\n",
    "# Load your trained model\n",
    "model = tf.keras.models.load_model(\"modelAdditionalLayers4001.keras\")\n",
    "\n",
    "# Initialize MediaPipe\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# ASL Labels (A-Z)\n",
    "labels = [chr(i) for i in range(ord('A'), ord('Z') + 1)]\n",
    "\n",
    "# ================== HAND PROCESSING ================== #\n",
    "def draw_hand_landmarks_on_black(hand_landmarks, image_size=128):\n",
    "    \"\"\"Convert MediaPipe landmarks to a black image with green connections\"\"\"\n",
    "    landmarks = np.array([[lm.x, lm.y] for lm in hand_landmarks.landmark])\n",
    "    x_vals, y_vals = landmarks[:, 0], landmarks[:, 1]\n",
    "    \n",
    "    # Normalize landmarks\n",
    "    min_x, max_x = np.min(x_vals), np.max(x_vals)\n",
    "    min_y, max_y = np.min(y_vals), np.max(y_vals)\n",
    "    width, height = max_x - min_x, max_y - min_y\n",
    "    scale = 0.7 * image_size / max(width, height)\n",
    "    \n",
    "    landmarks[:, 0] = (landmarks[:, 0] - min_x) * scale\n",
    "    landmarks[:, 1] = (landmarks[:, 1] - min_y) * scale\n",
    "    landmarks[:, 0] += (image_size - width * scale) / 2\n",
    "    landmarks[:, 1] += (image_size - height * scale) / 2\n",
    "\n",
    "    # Draw on black image\n",
    "    black_img = np.zeros((image_size, image_size, 3), dtype=np.uint8)\n",
    "    points = landmarks.astype(np.int32)\n",
    "    \n",
    "    # Draw connections\n",
    "    for connection in mp_hands.HAND_CONNECTIONS:\n",
    "        start = tuple(points[connection[0]])\n",
    "        end = tuple(points[connection[1]])\n",
    "        cv2.line(black_img, start, end, (0, 255, 0), 1)\n",
    "    \n",
    "    # Draw landmarks\n",
    "    for point in points:\n",
    "        cv2.circle(black_img, tuple(point), 1, (0, 255, 0), -1)\n",
    "    \n",
    "    return black_img\n",
    "\n",
    "# ================== AUTO-CORRECTION ================== #\n",
    "# Common ASL misclassifications\n",
    "ASL_COMMON_ERRORS = {\n",
    "    'A': 'S', 'B': 'V', 'D': 'F', 'E': 'M',\n",
    "    'G': 'Q', 'I': 'J', 'K': 'P', 'M': 'N',\n",
    "    'S': 'A', 'V': 'B', 'F': 'D', 'N': 'M'\n",
    "}\n",
    "\n",
    "def simple_autocorrect(word):\n",
    "    \"\"\"Basic letter substitution without NLTK\"\"\"\n",
    "    return ''.join([ASL_COMMON_ERRORS.get(c, c) for c in word])\n",
    "\n",
    "def advanced_autocorrect(word):\n",
    "    \"\"\"Try TextBlob correction with fallback to simple\"\"\"\n",
    "    try:\n",
    "        tb = TextBlob(word)\n",
    "        if tb.words:\n",
    "            suggestion, confidence = tb.words[0].spellcheck()[0]\n",
    "            return suggestion if confidence > 0.7 else word\n",
    "    except:\n",
    "        pass\n",
    "    return simple_autocorrect(word)\n",
    "\n",
    "# ================== MAIN LOOP ================== #\n",
    "# Tracking variables\n",
    "sentence = \"\"\n",
    "current_letter = \"\"\n",
    "confirmed_letter = \"\"\n",
    "stable_count = 0\n",
    "last_confirmed_time = time.time()\n",
    "no_hand_start_time = None\n",
    "\n",
    "# Thresholds\n",
    "STABILITY_THRESHOLD = 3\n",
    "LETTER_COOLDOWN = 1.0\n",
    "NO_HAND_TIMEOUT = 2.0\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    prediction_text = \"Show hand sign...\"\n",
    "    current_time = time.time()\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        no_hand_start_time = None\n",
    "        \n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Process hand image\n",
    "            hand_img = draw_hand_landmarks_on_black(hand_landmarks)\n",
    "            gray_img = cv2.cvtColor(hand_img, cv2.COLOR_BGR2GRAY)\n",
    "            normalized_img = gray_img.astype(\"float32\") / 255.0\n",
    "            input_img = np.expand_dims(normalized_img, axis=(0, -1))\n",
    "            \n",
    "            # Get prediction\n",
    "            prediction = model.predict(input_img, verbose=0)\n",
    "            predicted_index = np.argmax(prediction)\n",
    "            confidence = prediction[0][predicted_index]\n",
    "\n",
    "            if confidence > 0.8:\n",
    "                predicted_letter = labels[predicted_index]\n",
    "                \n",
    "                # Track stability\n",
    "                if predicted_letter == confirmed_letter:\n",
    "                    stable_count += 1\n",
    "                else:\n",
    "                    stable_count = 0\n",
    "                    confirmed_letter = predicted_letter\n",
    "                \n",
    "                # Update display\n",
    "                if stable_count > STABILITY_THRESHOLD:\n",
    "                    prediction_text = f\"Letter: {confirmed_letter}\"\n",
    "                    current_letter = confirmed_letter\n",
    "                else:\n",
    "                    prediction_text = f\"Detecting: {confirmed_letter}\"\n",
    "                \n",
    "                # Add to sentence if stable\n",
    "                if (stable_count > STABILITY_THRESHOLD and \n",
    "                    (current_time - last_confirmed_time > LETTER_COOLDOWN)):\n",
    "                    sentence += confirmed_letter\n",
    "                    last_confirmed_time = current_time\n",
    "                    stable_count = 0\n",
    "\n",
    "            # Draw landmarks\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            cv2.imshow(\"Hand Input\", hand_img)\n",
    "            break\n",
    "    else:\n",
    "        # Handle no hand\n",
    "        if no_hand_start_time is None:\n",
    "            no_hand_start_time = current_time\n",
    "        elif current_time - no_hand_start_time > NO_HAND_TIMEOUT:\n",
    "            if not sentence.endswith(\" \"):\n",
    "                sentence += \" \"\n",
    "            no_hand_start_time = None\n",
    "\n",
    "    # Apply auto-correction\n",
    "    corrected_sentence = advanced_autocorrect(sentence)\n",
    "\n",
    "    # Display\n",
    "    cv2.putText(frame, prediction_text, (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    cv2.putText(frame, f\"Raw: {sentence}\", (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 1)\n",
    "    cv2.putText(frame, f\"Corrected: {corrected_sentence}\", (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "    cv2.imshow(\"ASL Translator\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45beb83a-f4ac-4473-addb-07a30d04f7e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70337d57-685d-4ba9-ae38-f3995e458e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aienv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

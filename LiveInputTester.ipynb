{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e91d8a6b-60d2-4796-ab5c-5a98d499bf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load your trained model\n",
    "model = tf.keras.models.load_model(\"modelAdditionalLayers5block.keras\")  # Change this to your model path\n",
    "\n",
    "# Initialize MediaPipe\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False,\n",
    "                       max_num_hands=1,\n",
    "                       min_detection_confidence=0.5,\n",
    "                       min_tracking_confidence=0.5)\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "labels = [chr(i) for i in range(ord('A'), ord('Z')+1)]  # ['A', 'B', ..., 'Z']\n",
    "\n",
    "def draw_hand_landmarks_on_black(hand_landmarks, image_size=128):\n",
    "    # Extract landmark coordinates\n",
    "    landmarks = np.array([[lm.x, lm.y] for lm in hand_landmarks.landmark])\n",
    "\n",
    "    # Normalize coordinates\n",
    "    x_vals = landmarks[:, 0]\n",
    "    y_vals = landmarks[:, 1]\n",
    "    min_x, max_x = np.min(x_vals), np.max(x_vals)\n",
    "    min_y, max_y = np.min(y_vals), np.max(y_vals)\n",
    "\n",
    "    width = max_x - min_x\n",
    "    height = max_y - min_y\n",
    "    scale = 0.8 * image_size / max(width, height)\n",
    "\n",
    "    # Apply scale and center\n",
    "    landmarks[:, 0] = (landmarks[:, 0] - min_x) * scale\n",
    "    landmarks[:, 1] = (landmarks[:, 1] - min_y) * scale\n",
    "\n",
    "    offset_x = (image_size - width * scale) / 2\n",
    "    offset_y = (image_size - height * scale) / 2\n",
    "    landmarks[:, 0] += offset_x\n",
    "    landmarks[:, 1] += offset_y\n",
    "\n",
    "    # Create black canvas (now with 3 channels for color)\n",
    "    black_img = np.zeros((image_size, image_size, 3), dtype=np.uint8)\n",
    "    landmark_points = landmarks.astype(np.int32)\n",
    "\n",
    "    # Draw connections (green, thinner lines)\n",
    "    for connection in mp_hands.HAND_CONNECTIONS:\n",
    "        start = tuple(landmark_points[connection[0]])\n",
    "        end = tuple(landmark_points[connection[1]])\n",
    "        cv2.line(black_img, start, end, (0, 255, 0), 1)  # Green, thickness=1\n",
    "\n",
    "    # Draw landmark points (green, smaller points)\n",
    "    for point in landmark_points:\n",
    "        cv2.circle(black_img, tuple(point), 1, (0, 255, 0), -1)  # Green, radius=1\n",
    "\n",
    "    return black_img\n",
    "\n",
    "# Start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    prediction_text = \"\"\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Preprocess hand to match training input\n",
    "            colored_img = draw_hand_landmarks_on_black(hand_landmarks, image_size=128)\n",
    "            \n",
    "            # Convert to grayscale for model input\n",
    "            gray_img = cv2.cvtColor(colored_img, cv2.COLOR_BGR2GRAY)\n",
    "            normalized_img = gray_img.astype(\"float32\") / 255.0\n",
    "            input_img = np.expand_dims(normalized_img, axis=(0, -1))  # Shape: (1, 128, 128, 1)\n",
    "\n",
    "            # Predict\n",
    "            prediction = model.predict(input_img, verbose=0)\n",
    "            predicted_index = np.argmax(prediction)\n",
    "            confidence = prediction[0][predicted_index]\n",
    "\n",
    "            if confidence > 0.8:\n",
    "                prediction_text = f\"Prediction: {labels[predicted_index]} ({confidence*100:.1f}%)\"\n",
    "            else:\n",
    "                prediction_text = \"Unsure\"\n",
    "\n",
    "            # Optional: draw hand landmarks on webcam feed\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Show model input for debug (now shows green landmarks)\n",
    "            cv2.imshow(\"Model Input\", colored_img)\n",
    "\n",
    "            break  # Only use the first hand\n",
    "\n",
    "    # Display the prediction\n",
    "    cv2.putText(frame, prediction_text, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.imshow(\"ASL Detection\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # ESC key\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "405963ba-5f1f-459e-ba37-3a0cff56955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MobileNetV2Architecture Live Test\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load your trained model (MobileNetV2 expects RGB input with 3 channels)\n",
    "model = tf.keras.models.load_model(\"MobileNetV21.keras\")\n",
    "\n",
    "# Initialize MediaPipe\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False,\n",
    "                       max_num_hands=1,\n",
    "                       min_detection_confidence=0.5,\n",
    "                       min_tracking_confidence=0.5)\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# List of labels (A-Z)\n",
    "labels = [chr(i) for i in range(ord('A'), ord('Z')+1)]  # ['A', 'B', ..., 'Z']\n",
    "\n",
    "def draw_hand_landmarks_on_black(hand_landmarks, image_size=128):\n",
    "    # Extract landmark coordinates\n",
    "    landmarks = np.array([[lm.x, lm.y] for lm in hand_landmarks.landmark])\n",
    "\n",
    "    # Normalize coordinates to fit within image size\n",
    "    x_vals = landmarks[:, 0]\n",
    "    y_vals = landmarks[:, 1]\n",
    "    min_x, max_x = np.min(x_vals), np.max(x_vals)\n",
    "    min_y, max_y = np.min(y_vals), np.max(y_vals)\n",
    "\n",
    "    width = max_x - min_x\n",
    "    height = max_y - min_y\n",
    "    scale = 0.8 * image_size / max(width, height)\n",
    "\n",
    "    landmarks[:, 0] = (landmarks[:, 0] - min_x) * scale\n",
    "    landmarks[:, 1] = (landmarks[:, 1] - min_y) * scale\n",
    "\n",
    "    offset_x = (image_size - width * scale) / 2\n",
    "    offset_y = (image_size - height * scale) / 2\n",
    "    landmarks[:, 0] += offset_x\n",
    "    landmarks[:, 1] += offset_y\n",
    "\n",
    "    # Create black canvas (RGB)\n",
    "    black_img = np.zeros((image_size, image_size, 3), dtype=np.uint8)\n",
    "    landmark_points = landmarks.astype(np.int32)\n",
    "\n",
    "    # Draw connections and points\n",
    "    for connection in mp_hands.HAND_CONNECTIONS:\n",
    "        start = tuple(landmark_points[connection[0]])\n",
    "        end = tuple(landmark_points[connection[1]])\n",
    "        cv2.line(black_img, start, end, (0, 255, 0), 1)\n",
    "\n",
    "    for point in landmark_points:\n",
    "        cv2.circle(black_img, tuple(point), 1, (0, 255, 0), -1)\n",
    "\n",
    "    return black_img\n",
    "\n",
    "# Start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    prediction_text = \"\"\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Get black canvas with green hand drawing\n",
    "            input_image = draw_hand_landmarks_on_black(hand_landmarks, image_size=128)\n",
    "\n",
    "            # Normalize and expand dims for MobileNetV2\n",
    "            normalized_img = input_image.astype(\"float32\") / 255.0\n",
    "            input_tensor = np.expand_dims(normalized_img, axis=0)  # Shape: (1, 128, 128, 3)\n",
    "\n",
    "            # Predict\n",
    "            prediction = model.predict(input_tensor, verbose=0)\n",
    "            predicted_index = np.argmax(prediction)\n",
    "            confidence = prediction[0][predicted_index]\n",
    "\n",
    "            if confidence > 0.8:\n",
    "                prediction_text = f\"Prediction: {labels[predicted_index]} ({confidence*100:.1f}%)\"\n",
    "            else:\n",
    "                prediction_text = \"Unsure\"\n",
    "\n",
    "            # Draw landmarks on original frame\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Show model input for debugging\n",
    "            cv2.imshow(\"Model Input\", input_image)\n",
    "\n",
    "            break  # Only process one hand\n",
    "\n",
    "    # Show prediction on webcam feed\n",
    "    cv2.putText(frame, prediction_text, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.imshow(\"ASL Detection\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # ESC to quit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "063891e5-e7e4-4ef2-a213-6954a93d650a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File not found: filepath=model11.keras. Please ensure the file is an accessible `.keras` zip file.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load your trained model\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel11.keras\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mchr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# ---------- Replace with path to your image ----------\u001b[39;00m\n",
      "File \u001b[1;32mD:\\AI\\Projects\\aienv\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:200\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(\n\u001b[0;32m    197\u001b[0m         filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m\n\u001b[0;32m    198\u001b[0m     )\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m     )\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile format not supported: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeras 3 only supports V3 `.keras` files and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmight have a different name).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    218\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: File not found: filepath=model11.keras. Please ensure the file is an accessible `.keras` zip file."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load your trained model\n",
    "model = tf.keras.models.load_model(\"model11.keras\")\n",
    "labels = [chr(i) for i in range(ord('A'), ord('Z')+1)]\n",
    "\n",
    "# ---------- Replace with path to your image ----------\n",
    "image_path = 'adcv.png'\n",
    "colored_img = cv2.imread(image_path)\n",
    "\n",
    "if colored_img is None:\n",
    "    print(\"Error: Could not load image.\")\n",
    "else:\n",
    "    # Convert to grayscale and preprocess (same as training)\n",
    "    gray_img = cv2.cvtColor(colored_img, cv2.COLOR_BGR2GRAY)\n",
    "    normalized_img = gray_img.astype(\"float32\") / 255.0\n",
    "    input_img = np.expand_dims(normalized_img, axis=(0, -1))  # Add batch and channel dims\n",
    "\n",
    "    # Predict\n",
    "    prediction = model.predict(input_img, verbose=0)\n",
    "    predicted_index = np.argmax(prediction)\n",
    "    confidence = prediction[0][predicted_index]\n",
    "\n",
    "    if confidence > 0.5:\n",
    "        prediction_text = f\"Prediction: {labels[predicted_index]} ({confidence*100:.1f}%)\"\n",
    "    else:\n",
    "        prediction_text = \"Unsure\"\n",
    "\n",
    "    # Show results\n",
    "    print(prediction_text)\n",
    "    cv2.imshow(\"Model Input\", colored_img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30025493-3dbe-4825-a6a3-47db9b919cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('jsms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49a8028-1659-42a1-ac57-4516ea6819f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6364cc-1c34-4184-9b01-e27faf55bd7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b81cb0-691e-49e1-bd0c-efb778fa4b10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
